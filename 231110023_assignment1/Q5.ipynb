{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram 1000 model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Define paths\n",
    "input_file = \"hi_100.txt\"  # Path to the text corpus file\n",
    "model_prefix = \"unigram1k\"      # Prefix for the trained model files\n",
    "vocab_size = 1000                # Vocabulary size\n",
    "model_type = \"unigram\"            # Model type (other options: \"bpe\", \"char\", \"word\")\n",
    "\n",
    "# Train SentencePiece model\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={input_file} --model_prefix={model_prefix} --vocab_size={vocab_size} --model_type={model_type}\"\n",
    ")\n",
    "\n",
    "# Load trained model\n",
    "\n",
    "def unigram_1k(corpus):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(\"unigram1k.model\")\n",
    "    tokens = sp.encode_as_pieces(corpus)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram 2000 model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Define paths\n",
    "input_file = \"hi_100.txt\"  # Path to the text corpus file\n",
    "model_prefix = \"unigram2k\"      # Prefix for the trained model files\n",
    "vocab_size = 2000                # Vocabulary size\n",
    "model_type = \"unigram\"            # Model type (other options: \"bpe\", \"char\", \"word\")\n",
    "\n",
    "# Train SentencePiece model\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    f\"--input={input_file} --model_prefix={model_prefix} --vocab_size={vocab_size} --model_type={model_type}\"\n",
    ")\n",
    "\n",
    "# Load trained model\n",
    "def unigram_2k(corpus):\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(\"unigram2k.model\")\n",
    "    tokens = sp.encode_as_pieces(corpus)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bpe 1k model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Path to your corpus\n",
    "corpus_file = \"hi_100.txt\"\n",
    "\n",
    "# Path to save the trained model\n",
    "model_prefix = \"hindi_bpe_model_1k\"\n",
    "\n",
    "# Define the parameters for training the BPE model\n",
    "vocab_size = 1000  # You can adjust this based on your corpus size and requirements\n",
    "model_type = \"bpe\"  # BPE model type\n",
    "\n",
    "# Train the BPE model\n",
    "spm.SentencePieceTrainer.train(input=corpus_file, model_prefix=model_prefix, vocab_size=vocab_size, model_type=model_type)\n",
    "def bpe_1k(corpus):\n",
    "    sp = spm.SentencePieceProcessor()   \n",
    "    sp.Load(\"hindi_bpe_model_1k.model\")\n",
    "    tokens = sp.encode_as_pieces(corpus)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bpe 2k model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Path to your corpus\n",
    "corpus_file = \"hi_100.txt\"\n",
    "\n",
    "# Path to save the trained model\n",
    "model_prefix = \"hindi_bpe_model_2k\"\n",
    "\n",
    "# Define the parameters for training the BPE model\n",
    "vocab_size = 2000  # You can adjust this based on your corpus size and requirements\n",
    "model_type = \"bpe\"  # BPE model type\n",
    "\n",
    "# Train the BPE model\n",
    "spm.SentencePieceTrainer.train(input=corpus_file, model_prefix=model_prefix, vocab_size=vocab_size, model_type=model_type)\n",
    "def bpe_2k(corpus):\n",
    "    sp = spm.SentencePieceProcessor()   \n",
    "    sp.Load(\"hindi_bpe_model_2k.model\")\n",
    "    tokens = sp.encode_as_pieces(corpus)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mbert Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the mBERT tokenizer\n",
    "tokenizer_mbert_1k = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "# Define the Hindi line\n",
    "def mBert_1k(corpus):\n",
    "    tokens = tokenizer_mbert_1k.tokenize(tokenizer_mbert_1k.decode(tokenizer_mbert_1k.encode(corpus, max_length=1000, truncation=True, padding='max_length')))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the mBERT tokenizer\n",
    "tokenizer_mbert_2k = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "# Define the Hindi line\n",
    "def mBert_2k(corpus):\n",
    "    tokens = tokenizer_mbert_2k.tokenize(tokenizer_mbert_2k.decode(tokenizer_mbert_2k.encode(corpus, max_length=2000, truncation=True, padding='max_length')))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indic Bert tokens genration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the IndicBERT tokenizer\n",
    "tokenizer_indic_bert = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "def indicBert_1k(corpus):\n",
    "    tokens = tokenizer_indic_bert.tokenize(tokenizer_indic_bert.decode(tokenizer_indic_bert.encode(corpus, max_length=1000, truncation=True)))\n",
    "    return tokens\n",
    "\n",
    "def indicBert_2k(corpus):\n",
    "    tokens = tokenizer_indic_bert.tokenize(tokenizer_indic_bert.decode(tokenizer_indic_bert.encode(corpus, max_length=2000, truncation=True)))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## White Space Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WhiteSpaceTokenizer(corpus):\n",
    "    tokens = corpus.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_white_space(tokens):\n",
    "    ans = []\n",
    "    for token in tokens:\n",
    "        # token.lstrip()\n",
    "        ans.append(token)\n",
    "        # print(token)\n",
    "    return ans\n",
    "\n",
    "matra_of_vowels = {'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॄ', 'ॅ', 'े', 'ै', 'ो', 'ौ', 'ं', 'ः', 'ँ'}\n",
    "\n",
    "# Set containing Hindi vowels\n",
    "hindi_vowels = {'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ए', 'ऐ', 'ओ', 'औ', 'अं', 'अः', 'अँ'}\n",
    "\n",
    "# Set containing Hindi consonants\n",
    "hindi_consonants = {'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण', \n",
    "                    'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', \n",
    "                    'श', 'ष', 'स', 'ह', 'क्ष', 'त्र', 'ज्ञ'}\n",
    "halant = \"्\"\n",
    "\n",
    "def clean_token(tokens1):\n",
    "    tokens = []\n",
    "    for word in tokens1:\n",
    "        s=\"\"\n",
    "        for c in word:\n",
    "            if (c not in matra_of_vowels) and (c not in hindi_consonants) and (c not in hindi_vowels) and c != halant: continue\n",
    "            else: s = s + c\n",
    "        if(len(s)): tokens.append(s)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prf(tokens, ground_truth):\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens[i])):\n",
    "            if tokens[i][j] in ground_truth[i]:\n",
    "                true_positive += 1\n",
    "            else:\n",
    "                false_positive += 1\n",
    "    \n",
    "    for i in range(len(ground_truth)):\n",
    "        for token in ground_truth[i]:\n",
    "            if token not in tokens[i]:\n",
    "                false_negative += 1\n",
    "\n",
    "    precision = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "\n",
    "    # Calculate F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram 1k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram 1k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['फ', 'िल', 'हा', 'ल', 'हरियाणा', 'में', 'लाख', 'ह', 'ेक्ट', 'े', 'य', 'र', 'में', 'ग', '्', 'वार', 'की', 'बु', 'आई', 'हुई', 'है', 'और', 'लाख', 'ह', 'ेक्ट', 'े', 'य', 'र', 'में', 'इस', 'की', 'बु', 'आई', 'का', 'ल', 'क्ष', '्य', 'रख', 'ा', 'गया', 'है', 'जबकि', 'पिछले', 'साल', 'की', 'स', 'मान', 'अ', 'व', 'धि', 'में', 'लाख', 'ह', 'ेक्ट', 'े', 'य', 'र', 'में', 'ग', '्', 'वार', 'ब', 'ो', 'या', 'गया', 'था'], ['ग', 'ा', 'ड', 'ियों', 'वा', 'ला', 'काम', 'तो', 'हो', 'गया', 'कोई', 'और', 'बात', 'कह', 'नी', 'हो', 'किसी', 'को', 'प्र', 'धा', 'न', 'जी', 'ने', 'एक', 'बार', 'फिर', 'सब', 'को', 'सं', 'ब', 'ो', 'ध', 'ित', 'करते', 'ह', 'ु', 'ये', 'कहा'], ['मैं', 'सि', 'ख', 'ा', 'को', 'बा', 'थ', 'र', 'ू', 'म', 'में', 'ले', 'गया', 'वहां', 'पर', 'मैं', 'ने', 'उसे', 'द', 'ि', 'वार', 'प', 'क', 'ड', 'वा', 'के', 'ख', 'डा', 'कर', 'दिया', 'फिर', 'सा', 'ब', 'ु', 'न', 'अपने', 'हाथ', 'में', 'ले', 'के', 'ख', 'ू', 'ब', 'झ', 'ा', 'ग', 'बना', 'या'], ['इस', 'क्षेत्र', 'में', 'पोस्ट', 'ग्र', 'ै', 'ज', 'ु', 'ए', 'शन', 'करने', 'के', 'क्या', 'फ', 'ाय', 'द', 'े', 'हैं'], ['आप', 'ने', 'अ', 'क्स', 'र', 'लोगों', 'से', 'सु', 'ना', 'होगा', 'कि', 'ल', 'ाइ', 'फ', 'में', 'हम', 'ेश', 'ा', 'खुश', 'रह', 'ना', 'चाहिए'], ['इस', 'बीच', 'ब', 'ली', 'व', 'ु', 'ड', 'एक', '्ट', '्रेस', 'स', 'नी', 'ल', 'िय', 'ो', 'नी', 'ने', 'सोशल', 'मीडिया', 'के', 'ज', 'र', 'िए', 'अपने', 'प', 'ति', 'डे', 'न', 'िय', 'ल', 'वे', 'ब', 'र', 'को', 'ब', 'ध', 'ाई', 'दी'], ['म', 'ज', 'े', 'की', 'बात', 'कि', 'मे', 'ले', 'का', 'समय', 'शा', 'म', 'आ', 'ठ', 'बजे', 'तक', 'का', 'है', 'लेकिन', 'छ', 'ह', 'के', 'बाद', 'यहां', 'स', 'न्न', 'ा', 'टा', 'छ', 'ा', 'जाता', 'है'], ['इस', 'खबर', 'को', 'अपने', 'मि', 'त्र', 'ों', 'से', 'सा', 'झ', 'ा', 'करें', 'के', 'न्द्र', 'ीय', 'विश्व', 'वि', 'द', '्य', 'ालय', 'सह', 'ित', 'हर', 'शहर', 'में', 'कई', 'योजना', 'एं', 'ज', 'मी', 'न', 'की', 'उपलब्ध', 'ता', 'न', 'होने', 'के', 'कारण', 'है', 'बंद', 'पड', 'ी', 'हैं', 'अ', 'र', 'वि', 'न्द', 'शर्मा', 'हि', 'मा', 'च', 'ल', 'विकास', 'शी', 'ल', 'प्रदेश', 'है', 'यह', 'ा', 'ँ', 'की', 'सरकार', 'तथा', 'जनता', 'इस', 'े', 'त', 'र', 'क्क', 'ी', 'के', 'शि', 'ख', 'र', 'तक', 'ले', 'जा', 'ना', 'चाह', 'ती', 'है', 'पर', 'न्त', 'ु', 'यह', 'ा', 'ँ', 'प्रतिशत', 'काम', 'भ', 'ू', 'म', 'ि', 'की', 'उप', 'ल', 'भ', 'ध', 'ता'], ['यहां', 'पी', 'एम', 'मोदी', 'ने', 'बडी', 'ग', 'र्म', 'ज', 'ो', 'शी', 'के', 'साथ', 'जा', 'पा', 'नी', 'पी', 'एम', 'का', 'स्व', 'ा', 'ग', 'त', 'किया'], ['छात्र', 'ों', 'के', 'अ', 'ध', '्य', 'य', 'न', 'स्तर', 'में', 'कम', 'ी', 'के', 'कारण', 'बडी', 'संख्या', 'में', 'छात्र', 'न', 'वी', 'ं', 'क', 'क्ष', 'ा', 'में', 'फ', 'े', 'ल', 'हो', 'ते', 'हैं'], ['विधानसभा', 'चुनाव', 'में', 'उम्मीद', 'से', 'बडी', 'जीत', 'के', 'बाद', 'पांच', 'मा', 'ह', 'में', 'ऐसा', 'क्या', 'हुआ', 'कि', 'लोकसभा', 'चुनाव', 'में', 'कांग्रेस', 'को', 'कर', 'ा', 'री', 'श', 'िक', 'स्त', 'मिल', 'ी'], ['कर', '्', 'ना', 'ट', 'क', 'राज्य', 'प्र', 'द', 'ू', 'ष', 'ण', 'नि', 'य', 'ं', 'त्र', 'ण', 'बोर्ड', 'द्वारा', 'नोट', 'िस', 'थ', 'मा', 'ए', 'जाने', 'के', 'बाद', 'वा', 'ह', 'न', 'क', 'ल', 'प', 'ु', 'र्ज', 'े', 'बनाने', 'वाली', 'कंपनी', 'ब', 'श', 'ने', 'अ', 'द', 'ू', 'ग', 'ो', 'दी', 'सं', 'य', 'ं', 'त्र', 'को', 'अ', 'स्थ', 'ाई', 'रूप', 'से', 'बंद', 'कर', 'दिया', 'है'], ['कार्यक्रम', 'को', 'सं', 'ब', 'ो', 'ध', 'ित', 'करते', 'हुए', 'ट', 'ा', 'टा', 'ट्र', 'स्ट', 'के', 'च', 'े', 'य', 'र', 'म', 'ै', 'न', 'र', 'त', 'न', 'ट', 'ा', 'टा', 'ने', 'कहा', 'कि', 'इस', 'आ', 'ंद', 'ो', 'ल', 'न', 'का', 'हिस्सा', 'बन', 'ने', 'का', 'उन्हें', 'विशेष', 'ाधिकार', 'प्राप्त', 'हुआ', 'स्व', 'च्छ', 'भारत', 'मि', 'शन', 'को', 'लेकर', 'प्रधानमंत्री', 'न', 'रे', 'न्द्र', 'मोदी', 'की', 'स', 'रा', 'ह', 'ना', 'करते', 'हुए', 'उन्होंने', 'कहा', 'कि', 'ट', 'ा', 'टा', 'ट्र', 'स्ट', 'इस', 'कार्यक्रम', 'पर', 'सरकार', 'के', 'साथ', 'काम', 'करने', 'के', 'लिए', 'प्रति', 'ब', 'द्ध', 'है'], ['इसके', 'बाद', 'ना', 'र', 'ाय', 'ण', 'ने', 'अपना', 'स्', 'क', 'ू', 'टर', 'महिला', 'को', 'दे', 'दिया', 'और', 'मे', 'क्र', 'ि', 'स', 'र्क', 'ल', 'नाम', 'की', 'जगह', 'जाने', 'को', 'कहा', 'और', 'ना', 'र', 'ाय', 'ण', 'खुद', 'उनकी', 'स्', 'क', 'ू', 'टी', 'को', 'ध', 'क्क', 'ा', 'दे', 'ते', 'हुए', 'उस', 'जगह', 'पहुंच', 'े'], ['लोग', 'दी', 'वा', 'ली', 'म', 'ना', 'रहे', 'हो', 'ते', 'हैं', 'पुलिस', 'क', 'र्म', 'ी', 'सुरक्षा', 'में', 'लग', 'े', 'हो', 'ते', 'हैं'], ['र', 'शी', 'दा', 'अपनी', 'एक', 'मा', 'त्र', 'गु', 'ड', 'िया', 'को', 'प', 'क', 'ड', 'े', 'ख', 'डी', 'थी', 'जो', 'तब', 'से', 'उसके', 'पास', 'थी', 'जब', 'उसकी', 'आ', 'य', 'ु', 'तीन', 'वर्ष', 'की', 'थी'], ['उनका', 'यह', 'भी', 'कह', 'ना', 'है', 'कि', 'में', 'भ', 'ले', 'न', 'मा', 'ज', 'पढ', 'ने', 'के', 'सब', 'ू', 'त', 'न', 'मिल', 'े', 'ह', 'ों', 'लेकिन', 'से', 'यहां', 'न', 'मा', 'ज', 'पढ', 'ी', 'गई', 'है'], ['स', 'र', 'का', 'घ', 'ा', 'ट', 'स', 'र', 'का', 'घ', 'ा', 'ट', 'के', 'विधायक', 'कर', '्', 'न', 'ल', 'इं', 'द', '्र', 'सिंह', 'और', 'भ', 'ा', 'ज', 'य', 'ु', 'म', 'ो', 'के', 'जिला', 'अध्यक्ष', 'र', 'ज', 'त', 'ठ', 'ा', 'क', 'ु', 'र', 'ने', 'प्रदेश', 'सरकार', 'द्वारा', 'रा', 'त', 'के', 'अ', 'ढ', 'ाई', 'बजे', 'धर्म', 'शा', 'ला', 'स्थित', 'अ', 'ंत', 'र', 'रा', 'ष्ट', '्र', 'ीय', 'क्रिकेट', 'स्ट', 'ेड', 'िय', 'म', 'हो', 'ट', 'ल', 'द', 'ि', 'पै', 'वे', 'ल', 'िय', 'न', 'और', 'क्रिकेट', 'अ', 'का', 'द', 'मी', 'ख', 'ो', 'ल', 'ने', 'के', 'लिए', 'ना', 'द', 'ौ', 'न', 'गु', 'म', '्', 'मा', 'और', 'ला', 'ल'], ['उनके', 'सम', 'क्ष', 'भाजपा', 'इ', 'य', 'ों', 'ने', 'ज', 'म', 'कर', 'भ', 'डा', 'स', 'निकाल', 'ी'], ['पु', 'ण', 'े', 'फ', '्र', 'ें', 'च', 'ाइ', 'जी', 'की', 'बोल', 'ी', 'लगा', 'ने', 'वाले', 'सम', 'ू', 'ह', 'का', 'हिस्सा', 'थे', 'अ', 'मी', 'न', 'मोदी'], ['आ', 'ष', 'ा', 'ढ', 'मा', 'ह', 'में', 'बार', 'िश', 'होने', 'से', 'ता', 'ला', 'ब', 'ों', 'में', 'मे', 'ढ', 'का', 'ें', 'की', 'ट', 'र्', 'र', 'टर', '्र', 'सु', 'न', 'ाई', 'पड', 'ने', 'लग', 'ती', 'थी', 'लेकिन', 'इस', 'बार', 'बे', 'ल्', 'हा', 'से', 'बाद', 'ल', 'इस', 'क', 'द', 'र', 'र', 'ू', 'ठ', 'े', 'रहे', 'कि', 'म', 'ई', 'और', 'ज', 'ू', 'न', 'मा', 'ह', 'में', 'सिर्फ', 'एक', 'मिल', 'ी', 'मी', 'टर', 'बार', 'िश', 'हो', 'स', 'की', 'ब', 'र', 'सा', 'ती', 'मे', 'ढ', 'क', 'दिख', 'े', 'ही', 'नहीं'], ['र', 'ंग', 'के', 'इस', 'प्र', 'कार', 'का', 'प्र', 'य', 'ो', 'ग', 'इस', 'बात', 'का', 'सू', 'च', 'क', 'है', 'कि', 'अ', 'ती', 'त', 'में', 'ड', '्र', 'िक', 'डे', 'नी', 'और', 'उसके', 'परिवार', 'वाले', 'अ', 'ती', 'त', 'में', 'दुनिया', 'को', 'का', 'ले', 'और', 'स', 'फ', 'े', 'द', 'र', 'ंग', 'में', 'देख', 'ते', 'थे'], ['में', 'भ', 'ड', 'के', 'सि', 'ख', 'विरोध', 'ी', 'द', 'ंग', 'ा', 'मामले', 'पर', 'को', 'र्ट', 'के', 'फ', 'ै', 'स', 'ले', 'के', 'बाद', 'राजनीति', 'ग', 'र', 'मा', 'गई', 'है', 'के', 'न्द्र', 'ीय', 'मंत्री', 'और', 'भाजपा', 'के', 'ने', 'ता', 'र', 'व', 'िश', 'ंक', 'र', 'प्र', 'सा', 'द', 'ने', 'इस', 'फ', 'ै', 'स', 'ले', 'के'], ['इस', 'ी', 'से', 'परेशान', 'हो', 'कर', 'बुधवार', 'को', 'पी', 'ड', 'ित', 'कि', 'सा', 'न', 'ज', 'य', 'वी', 'र', 'ब', 'ै', 'ल', 'गा', 'डी', 'पर', 'स', 'वार', 'हो', 'कर', 'मि', 'ट', '्ट', 'ी', 'के', 'ते', 'ल', 'की', 'कै', 'न', 'के', 'साथ', 'वहां', 'पहुंच', 'ा', 'और', 'ल', 'घ', 'ु', 'सचिव', 'ालय', 'के', 'पिछले', 'ग', 'ेट', 'के', 'सामने', 'अपने', 'श', 'री', 'र', 'पर', 'मि', 'ट', '्ट', 'ी', 'का', 'ते', 'ल', 'छ', 'ि', 'ड', 'क', 'कर', 'आ', 'त्', 'म', 'ह', 'त', '्या', 'का', 'प्रयास', 'किया'], ['वह', 'विधायक', 'जी', 'के', 'व', '्य', 'व', 'हा', 'र', 'से', 'अभि', 'भ', 'ू', 'त', 'हो', 'गया']]\n",
      "[['फिलहाल', 'हरियाणा में', '2.15 लाख हेक्टेयर में', 'ग्वार की बुआई हुई है', 'और', '3 लाख हेक्टेयर में', 'इसकी बुआई का लक्ष्य रखा गया है', 'जबकि', 'पिछले साल', 'की समान अवधि में', '2.5 लाख हेक्टेयर में', 'ग्वार बोया गया था।'], ['गाड़ियों वाला काम', 'तो हो गया', 'कोई और बात', 'कहनी हो', 'किसी को?\"', 'प्रधान जी ने', 'एक बार फिर', 'सबको', 'संबोधित करते हुये', 'कहा।'], ['मैं', 'सिखा को', 'बाथरूम में', 'ले गया.', 'वहां', 'पर', 'मैंने', 'उसे', 'दिवार पकडवा के', 'खड़ा', 'कर दिया.फिर', 'साबुन', 'अपने हाथ में', 'ले के', 'खूब', 'झाग ', 'बनाया.'], ['इस क्षेत्र', 'में', 'पोस्ट ग्रैजुएशन', 'करने के', 'क्या', 'फायदे', 'हैं?'], ['आपने', 'अक्सर', 'लोगों से', 'सुना होगा', 'कि', 'लाइफ में', 'हमेशा', 'खुश रहना', 'चाहिए।'], ['इस बीच', 'बॉलीवुड एक्ट्रेस', 'सनी लियोनी', 'ने', 'सोशल मीडिया', 'के जरिए', 'अपने पति', 'डेनियल वेबर', 'को', 'बधाई दी।'], ['मज़े की बात', 'कि', 'मेले का समय', 'शाम', 'आठ बजे तक', 'का', 'है', 'लेकिन', 'छह के बाद', 'यहां', 'सन्नाटा', 'छा जाता है।'], ['इस खबर को', 'अपने मित्रों से', 'साझा करें', 'केन्द्रीय विश्व विद्यालय सहित', 'हर शहर में', 'कई योजनाएं', 'जमीन की उपलब्धता न होने के कारण', 'बंद पड़ी हैं', '-अरविन्द शर्मा', 'हिमाचल विकासशील प्रदेश है', 'यहाँ की सरकार तथा जनता', 'इसे तरक्की के शिखर तक ले जाना चाहती है', 'परन्तु', 'यहाँ 99 प्रतिशत काम भूमि की उपलभधता'], ['यहां', 'पीएम मोदी ने', 'बड़ी गर्मजोशी के', 'साथ', 'जापानी पीएम का', 'स्वागत किया।'], ['छात्रों के', 'अध्ययन स्तर में', 'कमी के', 'कारण', 'बड़ी संख्या में', 'छात्र', 'नवीं कक्षा में', 'फेल होते हैं।'], ['विधानसभा चुनाव में', 'उम्मीद से', 'बड़ी जीत के बाद', 'पांच माह में', 'ऐसा', 'क्या', 'हुआ', 'कि', 'लोकसभा चुनाव में', 'कांग्रेस को', 'करारी शिकस्त मिली।'], ['कर्नाटक राज्य प्रदूषण नियंत्रण बोर्ड द्वारा', 'नोटिस थमाए जाने के बाद', 'वाहन कलपुर्जे बनाने वाली कंपनी बॉश ने', 'अदूगोदी संयंत्र को', 'अस्थाई रूप से ', 'बंद कर दिया है।'], ['कार्यक्रम को', 'संबोधित करते हुए', 'टाटा ट्रस्ट के चेयरमैन रतन टाटा ने', 'कहा', 'कि', 'इस आंदोलन का हिस्सा बनने का', 'उन्हें', 'विशेषाधिकार प्राप्त हुआ।', 'स्वच्छ भारत मिशन को', 'लेकर', 'प्रधानमंत्री नरेन्द्र मोदी की', 'सराहना करते हुए', 'उन्होंने', 'कहा', 'कि', 'टाटा ट्रस्ट इस कार्यक्रम पर', 'सरकार के साथ काम करने के लिए', 'प्रतिबद्ध है।'], ['इसके बाद', 'नारायण ने', 'अपना स्कूटर', 'महिला को', 'दे दिया', 'और', 'मेक्रि सर्कल नाम की जगह', 'जाने को', 'कहा', 'और', 'नारायण', 'खुद', 'उनकी स्कूटी', 'को', 'धक्का देते हुए', 'उस जगह पहुंचे।'], ['लोग', 'दीवाली', 'मना रहे होते हैं', 'पुलिसकर्मी', 'सुरक्षा में', 'लगे होते हैं।'], ['रशीदा', 'अपनी एकमात्र गुड़िया', 'को', 'पकड़े खड़ी थी', 'जो', 'तब से', 'उसके पास', 'थी', 'जब', 'उसकी आयु', 'तीन वर्ष की थी।'], ['उनका', 'यह', 'भी', 'कहना है', 'कि', '”1856-57 में', 'भले नमाज़ पढ़ने के', 'सबूत न मिले हों', 'लेकिन 1949 से', 'यहां', 'नमाज़ पढ़ी गई है।'], ['सरकाघाट', '—सरकाघाट के विधायक कर्नल इंद्र सिंह', 'और', 'भाजयुमो के जिला अध्यक्ष रजत ठाकुर', 'ने', 'प्रदेश सरकार द्वारा', 'रात के अढ़ाई बजे', 'धर्मशाला स्थित अंतरराष्ट्रीय क्रिकेट स्टेडियम होटल दि पैवेलियन और क्रिकेट अकादमी', 'खोलने के लिए', 'नादौन', 'गुम्मा और लाल…'], ['उनके', 'समक्ष', 'भाजपाइयों ने', 'जमकर', 'भड़ास', 'निकाली।'], ['पुणे फ्रेंचाइजी की', 'बोली लगाने वाले समूह का', 'हिस्सा थे', 'अमीन : मोदी'], ['आषाढ़ माह में', 'बारिश होने से', 'तालाबों में', 'मेढकाें की', 'टर्र-टर्र सुनाई पड़ने लगती थी', 'लेकिन', 'इस बार', 'बेल्हा से', 'बादल इस कदर रूठे रहे', 'कि', 'मई और जून माह में', 'सिर्फ एक मिलीमीटर बारिश हो सकी।', 'बरसाती मेढक', 'दिखे ही नहीं'], ['रंग के', 'इस प्रकार का', 'प्रयोग', 'इस बात का', 'सूचक है', 'कि', 'अतीत में', '\"ड्रिक', 'डेनी और उसके परिवार वाले\"', 'अतीत में', 'दुनिया को', 'काले और सफेद रंग में', 'देखते थे।'], ['1984 में', 'भड़के सिख विरोधी दंगा मामले पर', 'कोर्ट के', 'फैसले के', 'बाद', 'राजनीति', 'गरमा गई है।', 'केन्द्रीय मंत्री', 'और', 'भाजपा के नेता रविशंकर प्रसाद ने', 'इस फैसले के...'], ['इसी से परेशान होकर', 'बुधवार को', 'पीड़ित किसान जयवीर बैलगाड़ी पर', 'सवार होकर', 'मिट्टी के तेल की कैन के साथ', 'वहां पहुंचा', 'और लघु सचिवालय के पिछले गेट के सामने', 'अपने शरीर पर', 'मिट्टी का तेल छिडक़ कर', 'आत्महत्या का प्रयास किया।'], ['वह', 'विधायक जी', 'के', 'व्यवहार से', 'अभिभूत', 'हो गया']]\n",
      "25\n",
      "25\n",
      "(0.05862068965517241, 0.2518518518518518, 0.0951048951048951)\n"
     ]
    }
   ],
   "source": [
    "tokenss = []\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    \n",
    "    for line in file:\n",
    "        \n",
    "        tokens = unigram_1k(line)\n",
    "        tokens = clean_token(tokens)\n",
    "        tokenss.append(tokens)\n",
    "# print(tokenss)\n",
    "grnd_trth = []\n",
    "with open('ground_truth.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    # for line in file:\n",
    "    #     print(line)\n",
    "    #         # Print the list of words\n",
    "    vector = []\n",
    "    for line in file:\n",
    "        # Split the line by commas to create a list of strings (word groups)\n",
    "        word_groups = line.strip().split(',')\n",
    "        word_groups = clean_white_space(word_groups)\n",
    "\n",
    "        # print(word_groups)\n",
    "        # vector.append(word_groups)\n",
    "        # Process each word group\n",
    "        grnd_trth.append(word_groups)\n",
    "# print(grnd_trth)\n",
    "# print(len(grnd_trth))\n",
    "# print(len(tokenss))\n",
    "prf = find_prf(tokenss, grnd_trth)\n",
    "print(prf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unigram 2k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram 2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.07979274611398963, 0.2851851851851852, 0.12469635627530366)\n"
     ]
    }
   ],
   "source": [
    "tokenss = []\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    \n",
    "    for line in file:\n",
    "        \n",
    "        tokens = unigram_2k(line)\n",
    "        tokens = clean_token(tokens)\n",
    "        tokenss.append(tokens)\n",
    "# print(tokenss)\n",
    "grnd_trth = []\n",
    "with open('ground_truth.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    # for line in file:\n",
    "    #     print(line)\n",
    "    #         # Print the list of words\n",
    "    vector = []\n",
    "    for line in file:\n",
    "        # Split the line by commas to create a list of strings (word groups)\n",
    "        word_groups = line.strip().split(',')\n",
    "        word_groups = clean_white_space(word_groups)\n",
    "\n",
    "        # print(word_groups)\n",
    "        # vector.append(word_groups)\n",
    "        # Process each word group\n",
    "        grnd_trth.append(word_groups)\n",
    "prf = find_prf(tokenss, grnd_trth)\n",
    "print(prf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bpe_1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.05382674516400336, 0.2379182156133829, 0.0877914951989026)\n"
     ]
    }
   ],
   "source": [
    "tokenss = []\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    \n",
    "    for line in file:\n",
    "        \n",
    "        tokens = bpe_1k(line)\n",
    "        tokens = clean_token(tokens)\n",
    "        tokenss.append(tokens)\n",
    "# print(tokenss)\n",
    "grnd_trth = []\n",
    "with open('ground_truth.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    # for line in file:\n",
    "    #     print(line)\n",
    "    #         # Print the list of words\n",
    "    vector = []\n",
    "    for line in file:\n",
    "        # Split the line by commas to create a list of strings (word groups)\n",
    "        word_groups = line.strip().split(',')\n",
    "        word_groups = clean_white_space(word_groups)\n",
    "\n",
    "        # print(word_groups)\n",
    "        # vector.append(word_groups)\n",
    "        # Process each word group\n",
    "        grnd_trth.append(word_groups)\n",
    "prf = find_prf(tokenss, grnd_trth)\n",
    "print(prf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bpe_2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.06965174129353234, 0.26022304832713755, 0.10989010989010989)\n"
     ]
    }
   ],
   "source": [
    "tokenss = []\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    \n",
    "    for line in file:\n",
    "        \n",
    "        tokens = bpe_2k(line)\n",
    "        tokens = clean_token(tokens)\n",
    "        tokenss.append(tokens)\n",
    "# print(tokenss)\n",
    "grnd_trth = []\n",
    "with open('ground_truth.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    # for line in file:\n",
    "    #     print(line)\n",
    "    #         # Print the list of words\n",
    "    vector = []\n",
    "    for line in file:\n",
    "        # Split the line by commas to create a list of strings (word groups)\n",
    "        word_groups = line.strip().split(',')\n",
    "        word_groups = clean_white_space(word_groups)\n",
    "\n",
    "        # print(word_groups)\n",
    "        # vector.append(word_groups)\n",
    "        # Process each word group\n",
    "        grnd_trth.append(word_groups)\n",
    "prf = find_prf(tokenss, grnd_trth)\n",
    "print(prf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mbert 1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.05067567567567568, 0.2247191011235955, 0.08270158511371468)\n"
     ]
    }
   ],
   "source": [
    "tokenss = []\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    \n",
    "    for line in file:\n",
    "        \n",
    "        tokens = mBert_1k(line)\n",
    "        tokens = clean_token(tokens)\n",
    "        tokenss.append(tokens)\n",
    "# print(tokenss)\n",
    "grnd_trth = []\n",
    "with open('ground_truth.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    # for line in file:\n",
    "    #     print(line)\n",
    "    #         # Print the list of words\n",
    "    vector = []\n",
    "    for line in file:\n",
    "        # Split the line by commas to create a list of strings (word groups)\n",
    "        word_groups = line.strip().split(',')\n",
    "        word_groups = clean_white_space(word_groups)\n",
    "\n",
    "        # print(word_groups)\n",
    "        # vector.append(word_groups)\n",
    "        # Process each word group\n",
    "        grnd_trth.append(word_groups)\n",
    "prf = find_prf(tokenss, grnd_trth)\n",
    "print(prf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M bert 2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.05067567567567568, 0.2247191011235955, 0.08270158511371468)\n"
     ]
    }
   ],
   "source": [
    "tokenss = []\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    \n",
    "    for line in file:\n",
    "        \n",
    "        tokens = mBert_2k(line)\n",
    "        tokens = clean_token(tokens)\n",
    "        tokenss.append(tokens)\n",
    "# print(tokenss)\n",
    "grnd_trth = []\n",
    "with open('ground_truth.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    # for line in file:\n",
    "    #     print(line)\n",
    "    #         # Print the list of words\n",
    "    vector = []\n",
    "    for line in file:\n",
    "        # Split the line by commas to create a list of strings (word groups)\n",
    "        word_groups = line.strip().split(',')\n",
    "        word_groups = clean_white_space(word_groups)\n",
    "\n",
    "        # print(word_groups)\n",
    "        # vector.append(word_groups)\n",
    "        # Process each word group\n",
    "        grnd_trth.append(word_groups)\n",
    "prf = find_prf(tokenss, grnd_trth)\n",
    "print(prf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# indic bert 1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.013171225937183385, 0.04961832061068702, 0.020816653322658127)\n"
     ]
    }
   ],
   "source": [
    "tokenss = []\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    \n",
    "    for line in file:\n",
    "        \n",
    "        tokens = indicBert_1k(line)\n",
    "        tokens = clean_token(tokens)\n",
    "        tokenss.append(tokens)\n",
    "# print(tokenss)\n",
    "grnd_trth = []\n",
    "with open('ground_truth.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    # for line in file:\n",
    "    #     print(line)\n",
    "    #         # Print the list of words\n",
    "    vector = []\n",
    "    for line in file:\n",
    "        # Split the line by commas to create a list of strings (word groups)\n",
    "        word_groups = line.strip().split(',')\n",
    "        word_groups = clean_white_space(word_groups)\n",
    "\n",
    "        # print(word_groups)\n",
    "        # vector.append(word_groups)\n",
    "        # Process each word group\n",
    "        grnd_trth.append(word_groups)\n",
    "prf = find_prf(tokenss, grnd_trth)\n",
    "print(prf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# indic bert 2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.013171225937183385, 0.04961832061068702, 0.020816653322658127)\n"
     ]
    }
   ],
   "source": [
    "tokenss = []\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    \n",
    "    for line in file:\n",
    "        \n",
    "        tokens = indicBert_2k(line)\n",
    "        tokens = clean_token(tokens)\n",
    "        tokenss.append(tokens)\n",
    "# print(tokenss)\n",
    "grnd_trth = []\n",
    "with open('ground_truth.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    # for line in file:\n",
    "    #     print(line)\n",
    "    #         # Print the list of words\n",
    "    vector = []\n",
    "    for line in file:\n",
    "        # Split the line by commas to create a list of strings (word groups)\n",
    "        word_groups = line.strip().split(',')\n",
    "        word_groups = clean_white_space(word_groups)\n",
    "\n",
    "        # print(word_groups)\n",
    "        # vector.append(word_groups)\n",
    "        # Process each word group\n",
    "        grnd_trth.append(word_groups)\n",
    "prf = find_prf(tokenss, grnd_trth)\n",
    "print(prf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WhiteSpaceTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.13971742543171115, 0.3308550185873606, 0.19646799116997793)\n"
     ]
    }
   ],
   "source": [
    "tokenss = []\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    \n",
    "    for line in file:\n",
    "        \n",
    "        tokens = WhiteSpaceTokenizer(line)\n",
    "        tokens = clean_token(tokens)\n",
    "        tokenss.append(tokens)\n",
    "# print(tokenss)\n",
    "grnd_trth = []\n",
    "with open('ground_truth.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the entire content of the file\n",
    "    # for line in file:\n",
    "    #     print(line)\n",
    "    #         # Print the list of words\n",
    "    vector = []\n",
    "    for line in file:\n",
    "        # Split the line by commas to create a list of strings (word groups)\n",
    "        word_groups = line.strip().split(',')\n",
    "        word_groups = clean_white_space(word_groups)\n",
    "\n",
    "        # print(word_groups)\n",
    "        # vector.append(word_groups)\n",
    "        # Process each word group\n",
    "        grnd_trth.append(word_groups)\n",
    "prf = find_prf(tokenss, grnd_trth)\n",
    "print(prf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
